{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzzmKbHhZotg75ekPKu4jv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasquemelli/Star_Jeans/blob/main/webscraping/webscraping_H%26M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNQkKS44gx4S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import re\n",
        "import math\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection (products)"
      ],
      "metadata": {
        "id": "QC3wAIXzg6qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL\n",
        "url01 = \"https://www2.hm.com/en_us/men/products/jeans.html?sort=stock&image-size=small&image=model&offset=0&page-size=72\"\n",
        "\n",
        "# Parameters\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "\n",
        "# Request to URL\n",
        "page = requests.get(url01, headers=headers)\n",
        "\n",
        "# Beautiful Soup object\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# ========================= Product Data ====================== #\n",
        "\n",
        "# List which contains all products\n",
        "products = soup.find('ul', 'products-listing small')\n",
        "  \n",
        "# product_id_categort list\n",
        "product_id_category = products.find_all('article', 'hm-product-item')\n",
        "\n",
        "# product_name list\n",
        "product_name = products.find_all('a', 'link')\n",
        "\n",
        "# product_price list\n",
        "product_price = products.find_all('span', 'price regular')\n",
        "\n",
        "product_id = [p.get('data-articlecode') for p in product_id_category]\n",
        "product_category = [p.get('data-category') for p in product_id_category]\n",
        "product_name = [p.get_text() for p in product_name]\n",
        "product_price = [p.get_text() for p in product_price]\n",
        "\n",
        "data = pd.DataFrame([product_id, product_name, product_category, product_price]).T\n",
        "data.columns = ['product_id', 'product_name', 'product_type', 'price']\n",
        "\n",
        "data['scrape_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
      ],
      "metadata": {
        "id": "kFSERSf-g4eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection (inside each product)"
      ],
      "metadata": {
        "id": "Fqgb3Mr1hAOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "\n",
        "#empty dataframe\n",
        "df_final = pd.DataFrame()\n",
        "\n",
        "# Auxiliar list in order to monitor new columns\n",
        "aux = []\n",
        "\n",
        "# All columns found on website\n",
        "df_pattern = pd.DataFrame(columns= ['Art. No.', 'Composition', 'Fit', 'More sustainable materials', 'Size'])\n",
        "\n",
        "for i in range(len(data)):\n",
        "\n",
        "    #API request\n",
        "    # conteúdo de headers é padrão\n",
        "    url02 = \"https://www2.hm.com/en_us/productpage.\" + data.loc[i, 'product_id'] + \".html\"\n",
        "\n",
        "    page = requests.get(url02, headers=headers)\n",
        "\n",
        "    #Beautiful Soup object\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    # ============================= Color =========================\n",
        "\n",
        "    #product list\n",
        "    product_list = soup.find_all('a', class_='filter-option miniature active') + soup.find_all('a', class_='filter-option miniature')\n",
        "\n",
        "    #color\n",
        "    product_color = [p.get('data-color') for p in product_list] \n",
        "\n",
        "    #id\n",
        "    product_id = [p.get('data-articlecode') for p in product_list]\n",
        "\n",
        "    #dataframe\n",
        "    df_color = pd.DataFrame([product_id, product_color]).T\n",
        "    df_color.columns = ['product_id', 'color']\n",
        "\n",
        "    for j in range(len(df_color)):\n",
        "        \n",
        "        # ============== API request ========================= \n",
        "        \n",
        "        # conteúdo de headers é padrão\n",
        "        url03 = \"https://www2.hm.com/en_us/productpage.\" + df_color.loc[j, 'product_id'] + \".html\"\n",
        "\n",
        "        page = requests.get(url03, headers=headers)\n",
        "\n",
        "        #Beautiful Soup object\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')\n",
        "        \n",
        "        # ============== Product name ==========================\n",
        "        \n",
        "        product_name = soup.find('section', class_ = 'product-name-price').find_all('h1')\n",
        "        product_name = product_name[0].get_text()\n",
        "        \n",
        "        # ============== Product price =========================\n",
        "        \n",
        "        product_price = soup.find_all('div', class_ = 'primary-row product-item-price')\n",
        "        product_price = re.findall(r'\\d+\\.?\\d+', product_price[0].get_text())[0]\n",
        "        \n",
        "\n",
        "        # ============================ Composition =====================\n",
        "\n",
        "        # Product list -- we used find and find_all because we could not return composition list only by using find_all in the beginning\n",
        "        product_composition_list = soup.find('div', class_='content pdp-text pdp-content').find_all('div')\n",
        "\n",
        "        # Composition\n",
        "        product_composition = [list( filter( None, p.get_text().split('\\n') ) ) for p in product_composition_list]\n",
        "\n",
        "        # dataframe\n",
        "        df_composition = pd.DataFrame(product_composition).T\n",
        "\n",
        "        # Columns name\n",
        "        df_composition.columns = df_composition.iloc[0]\n",
        "\n",
        "        # Filling None/NA values\n",
        "        df_composition = df_composition.iloc[1:].fillna(method='ffill')\n",
        "\n",
        "        # Removing pocket lining, shell and lining\n",
        "        df_composition['Composition'] = df_composition['Composition'].str.replace('Pocket lining: ', '', regex=True)\n",
        "        df_composition['Composition'] = df_composition['Composition'].str.replace('Shell: ', '', regex=True)\n",
        "        df_composition['Composition'] = df_composition['Composition'].str.replace('Lining: ', '', regex=True)\n",
        "\n",
        "        # The same number of columns (pattern)\n",
        "        df_composition = pd.concat( [df_pattern, df_composition] )\n",
        "        \n",
        "        # Rename columns\n",
        "        if j == 0:\n",
        "            df_composition.columns = ['product_id', 'composition', 'fit', 'product_safety', 'size']\n",
        "            df_color['product_id'] = df_color['product_id'].astype(str)\n",
        "            df_composition['product_id'] = df_composition['product_id'].astype(str)\n",
        "            \n",
        "        else:\n",
        "            break\n",
        "\n",
        "        # Keep new columns if it shows up\n",
        "        aux = aux + df_composition.columns.tolist()\n",
        "\n",
        "        data_merge = pd.merge(df_composition[['product_id', 'composition', 'fit', 'product_safety', 'size']], df_color, \n",
        "                              how='left', on='product_id')\n",
        "        data_merge.loc[j, 'product_name'] = product_name\n",
        "        data_merge.loc[j, 'product_price'] = product_price\n",
        "        \n",
        "        # ======================= Concatenate ==========================================\n",
        "        df_final = pd.concat( [df_final, data_merge], axis=0 )\n",
        "\n",
        "# Creating style_id + color_id\n",
        "df_final['style_id'] = df_final['product_id'].apply(lambda x: x[:-3])\n",
        "df_final['color_id'] = df_final['product_id'].apply(lambda x: x[-3:])\n",
        "\n",
        "df_final['scrape_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "data_raw = df_final.copy().reset_index().drop(columns='index')\n",
        "\n",
        "data_raw.to_csv(\"data_raw.csv\")"
      ],
      "metadata": {
        "id": "koD2a6RKhJso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "QB-3v5kphrKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_raw.dropna(subset=['product_id'])\n",
        "\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "#data['product_price'] = data['product_price'].apply(lambda x: x.replace('$ ', '')).astype(str)\n",
        "data['product_price'] = data['product_price'].astype(float)\n",
        "\n",
        "data['scrape_datetime'] = pd.to_datetime(data['scrape_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "data['color'] = data['color'].apply(lambda x: x.replace(' ', '_').replace('/', '_').replace('-', '_').lower() )\n",
        "\n",
        "data['fit'] = data['fit'].apply(lambda x: x.replace(' ', '_').lower() ) \n",
        "\n",
        "data['product_name'] = data['product_name'].astype(str)\n",
        "data['product_name'] = data['product_name'].apply(lambda x: x.replace(' ', '_').replace(':', '').replace('®', '').replace('-', '_').lower() )\n",
        "\n",
        "data = data[~data['composition'].str.contains('Pocket lining:')]\n",
        "data = data[~data['composition'].str.contains('Lining:')]\n",
        "data = data[~data['composition'].str.contains('Shell:')]\n",
        "data = data[~data['composition'].str.contains('Pocket:')]\n",
        "\n",
        "df1 = data['composition'].str.split(',', expand=True).reset_index(drop=True)\n",
        "\n",
        "df_ref = pd.DataFrame(index=np.arange(len(data)), columns=['Cotton', 'Polyester', 'Spandex', 'Lyocell', \n",
        "                                                          'Rayon', 'Elastomultiester'])\n",
        "\n",
        "# ================================ Cotton =============================\n",
        "df_cotton_0 = df1.loc[df1[0].str.contains('Cotton', na=True), 0]\n",
        "df_cotton_0.name = 'cotton'\n",
        "df_cotton_1 = df1.loc[df1[1].str.contains('Cotton', na=True), 1]\n",
        "df_cotton_1.name = 'cotton'\n",
        "\n",
        "df_cotton = df_cotton_0.combine_first(df_cotton_1)\n",
        "\n",
        "df_ref = pd.concat([df_ref, df_cotton], axis=1)\n",
        "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated()]\n",
        "df_ref = df_ref.drop(columns=['Cotton'], axis=1) \n",
        "df_ref['cotton'] = df_ref['cotton'].fillna('Cotton 0%')\n",
        "\n",
        "# ============================== Polyester =============================\n",
        "df_polyester_0 = df1.loc[df1[0].str.contains('Polyester', na=True), 0]\n",
        "df_polyester_0.name = 'polyester'\n",
        "df_polyester_1 = df1.loc[df1[1].str.contains('Polyester', na=True), 1]\n",
        "df_polyester_1.name = 'polyester'\n",
        "\n",
        "df_polyester = df_polyester_0.combine_first(df_polyester_1)\n",
        "\n",
        "df_ref = pd.concat([df_ref, df_polyester], axis=1)\n",
        "df_ref = df_ref.drop(columns=['Polyester'], axis=1)\n",
        "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated()] \n",
        "df_ref['polyester'] = df_ref['polyester'].fillna('Polyester 0%')\n",
        "\n",
        "# =============================== Spandex ===============================\n",
        "df_spandex_1 = df1.loc[df1[1].str.contains('Spandex', na=True), 1]\n",
        "df_spandex_1.name = 'spandex'\n",
        "df_spandex_2 = df1.loc[df1[2].str.contains('Spandex', na=True), 2]\n",
        "df_spandex_2.name = 'spandex'\n",
        "\n",
        "df_spandex = df_spandex_1.combine_first(df_spandex_2)\n",
        "\n",
        "df_ref = pd.concat([df_ref, df_spandex], axis=1)\n",
        "df_ref = df_ref.drop(columns=['Spandex'], axis=1)\n",
        "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated()]\n",
        "df_ref['spandex'] = df_ref['spandex'].fillna('Spandex 0%')\n",
        "\n",
        "# ============================== Lyocell ================================\n",
        "df_lyocell_0 = df1.loc[df1[0].str.contains('Lyocell', na=True), 0]\n",
        "df_lyocell_0.name = 'lyocell'\n",
        "df_lyocell_1 = df1.loc[df1[1].str.contains('Lyocell', na=True), 1]\n",
        "df_lyocell_1.name = 'lyocell' \n",
        "\n",
        "df_lyocell = df_lyocell_0.combine_first(df_lyocell_1)\n",
        "\n",
        "df_ref = pd.concat([df_ref, df_lyocell], axis=1)\n",
        "df_ref = df_ref.drop(columns=['Lyocell'], axis=1)\n",
        "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated()]\n",
        "df_ref['lyocell'] = df_ref['lyocell'].fillna('Lyocell 0%')\n",
        "\n",
        "# ============================== Rayon ================================\n",
        "df_rayon_0 = df1.loc[df1[0].str.contains('Rayon', na=True), 0]\n",
        "df_rayon_0.name = 'rayon'\n",
        "df_rayon_2 = df1.loc[df1[2].str.contains('Rayon', na=True), 2]\n",
        "df_rayon_2.name = 'rayon' \n",
        "\n",
        "df_rayon = df_rayon_0.combine_first(df_rayon_2)\n",
        "\n",
        "df_ref = pd.concat([df_ref, df_rayon], axis=1)\n",
        "df_ref = df_ref.drop(columns=['Rayon'], axis=1)\n",
        "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated()]\n",
        "df_ref['rayon'] = df_ref['rayon'].fillna('Rayon 0%')\n",
        "\n",
        "# ============================== Elastomultiester ================================\n",
        "df_elastomultiester = df1.loc[df1[1].str.contains('Elastomultiester', na=True), 1]\n",
        "df_elastomultiester.name = 'elastomultiester'\n",
        "\n",
        "df_ref = pd.concat([df_ref, df_elastomultiester], axis=1)\n",
        "df_ref = df_ref.drop(columns=['Elastomultiester'], axis=1)\n",
        "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated()]\n",
        "df_ref['elastomultiester'] = df_ref['elastomultiester'].fillna('Elastomultiester 0%')\n",
        "\n",
        "# ========================= Including new columns on data dataframe =======================\n",
        "data = pd.concat([data.reset_index(), df_ref.reset_index()], axis=1)\n",
        "data = data.drop(columns=['index'], axis=1)\n",
        "data = data.iloc[:, ~data.columns.duplicated()]\n",
        "\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# ============== join of combine with product_id ==============\n",
        "\n",
        "df_aux = pd.concat( [data['product_id'].reset_index(drop=True), df_ref], axis=1 )\n",
        "\n",
        "\n",
        "# =============== Excluding strings in order to keep only numbers for composition columns ==============\n",
        "\n",
        "df_aux['cotton'] = df_aux['cotton'].apply(lambda x: int( re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
        "df_aux['polyester'] = df_aux['polyester'].apply(lambda x: int( re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
        "df_aux['spandex'] = df_aux['spandex'].apply(lambda x: int( re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
        "df_aux['lyocell'] = df_aux['lyocell'].apply(lambda x: int( re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
        "df_aux['rayon'] = df_aux['rayon'].apply(lambda x: int( re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
        "df_aux['elastomultiester'] = df_aux['elastomultiester'].apply(lambda x: int( re.search('\\d+', x).group(0))/100 if pd.notnull(x) else x)\n",
        "\n",
        "df_aux = df_aux.groupby('product_id').max().reset_index().fillna(0)\n",
        "data = data.drop(columns=['cotton', 'polyester', 'spandex', 'lyocell', 'rayon', 'elastomultiester'])\n",
        "data = pd.merge( data, df_aux, on='product_id', how='left' )\n",
        "\n",
        "# ====== inserting model_size and jeans_size instead of size =========\n",
        "\n",
        "data['model_size'] = data['size'].apply(lambda x: re.search('\\d{3}', x).group(0) if pd.notnull(x) else x).astype(float)\n",
        "data['jeans_size'] = data['size'].str.extract('(\\d+/\\\\d+)')\n",
        "       \n",
        "data = data.drop(columns=['size', 'product_safety', 'composition'], axis=1).reset_index(drop=True)\n",
        "#data = data.drop_duplicates(subset=['product_id', 'fit', 'color', 'style_id', 'color_id', \n",
        "#                                    'model_size', 'jeans_size'], keep='first')\n",
        "\n",
        "data.to_csv(\"data_clean.csv\")"
      ],
      "metadata": {
        "id": "UXtKPhtlhsqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}